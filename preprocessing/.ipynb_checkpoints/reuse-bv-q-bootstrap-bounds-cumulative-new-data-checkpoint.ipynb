{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# To-Dos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Outlier Removal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "check.packages = function(pkg) {\n",
    "    new.pkg = pkg[!(pkg %in% installed.packages()[, \"Package\"])]\n",
    "    if (length(new.pkg)) \n",
    "        install.packages(new.pkg, dependencies = TRUE)\n",
    "    sapply(pkg, require, character.only = TRUE)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<dl class=dl-horizontal>\n",
       "\t<dt>RPostgreSQL</dt>\n",
       "\t\t<dd>TRUE</dd>\n",
       "\t<dt>RJDBC</dt>\n",
       "\t\t<dd>TRUE</dd>\n",
       "\t<dt>ggplot2</dt>\n",
       "\t\t<dd>TRUE</dd>\n",
       "\t<dt>dplyr</dt>\n",
       "\t\t<dd>TRUE</dd>\n",
       "\t<dt>gridExtra</dt>\n",
       "\t\t<dd>TRUE</dd>\n",
       "\t<dt>cowplot</dt>\n",
       "\t\t<dd>TRUE</dd>\n",
       "\t<dt>rJava</dt>\n",
       "\t\t<dd>TRUE</dd>\n",
       "</dl>\n"
      ],
      "text/latex": [
       "\\begin{description*}\n",
       "\\item[RPostgreSQL] TRUE\n",
       "\\item[RJDBC] TRUE\n",
       "\\item[ggplot2] TRUE\n",
       "\\item[dplyr] TRUE\n",
       "\\item[gridExtra] TRUE\n",
       "\\item[cowplot] TRUE\n",
       "\\item[rJava] TRUE\n",
       "\\end{description*}\n"
      ],
      "text/markdown": [
       "RPostgreSQL\n",
       ":   TRUERJDBC\n",
       ":   TRUEggplot2\n",
       ":   TRUEdplyr\n",
       ":   TRUEgridExtra\n",
       ":   TRUEcowplot\n",
       ":   TRUErJava\n",
       ":   TRUE\n",
       "\n"
      ],
      "text/plain": [
       "RPostgreSQL       RJDBC     ggplot2       dplyr   gridExtra     cowplot \n",
       "       TRUE        TRUE        TRUE        TRUE        TRUE        TRUE \n",
       "      rJava \n",
       "       TRUE "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "check.packages(c('RPostgreSQL', 'RJDBC', 'ggplot2', 'dplyr', 'gridExtra', 'cowplot', 'rJava'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "options(scipen = 999)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Connect to Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "killDbConnections = function () {\n",
    "  all_cons = dbListConnections(PostgreSQL())\n",
    "  print(all_cons)\n",
    "  for(con in all_cons)\n",
    "    +  dbDisconnect(con)\n",
    "  \n",
    "#   print(paste(length(all_cons), \" connections killed.\"))\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "createReportingConnectionObject = function() {\n",
    "    \n",
    "    killDbConnections()\n",
    "    \n",
    "    drv = dbDriver('PostgreSQL')\n",
    "\n",
    "    reporting = dbConnect(\n",
    "      drv,\n",
    "      dbname = 'reporting',\n",
    "      host = 'reporting.ckpglb17yttu.us-east-1.rds.amazonaws.com',\n",
    "      port = 5432,\n",
    "      user = Sys.getenv('REPORTING_USER'),\n",
    "      password = Sys.getenv('REPORTING_PASS')\n",
    "    )\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "checkReportingConnection = function() {\n",
    "    \n",
    "    print(paste(length(dbListConnections(PostgreSQL())), \" connections active.\"))\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "getDf = function(date.string, ibv.provider) {\n",
    "    \n",
    "    df = dbGetQuery(\n",
    "        createReportingConnectionObject(),\n",
    "        paste(\n",
    "            \"\n",
    "            with time_limit_gmt as \n",
    "            (\n",
    "              select\n",
    "                date_trunc('hour', min(createddate_)) as createddate_\n",
    "              from\n",
    "                (\n",
    "                  select\n",
    "                    max(createddate) as createddate_\n",
    "                  from\n",
    "                    cloudlending.application_status_history\n",
    "\n",
    "                  union all\n",
    "\n",
    "                  select\n",
    "                    max(createddate) as createddate_\n",
    "                  from\n",
    "                    cloudlending.microbilt_information\n",
    "                ) as latest_dates\n",
    "            )\n",
    "            , qualified as\n",
    "            (\n",
    "              select\n",
    "                application\n",
    "                , min(createddate at time zone 'America/Chicago') as createddate_\n",
    "              from\n",
    "                cloudlending.application_status_history\n",
    "                inner join\n",
    "                  time_limit_gmt\n",
    "                  on TRUE\n",
    "              where\n",
    "                createddate < createddate_\n",
    "                and old_value = 'BUSINESS RULES PASSED'\n",
    "                and new_value = 'BUREAU APPROVED'\n",
    "              group by\n",
    "                application\n",
    "            )\n",
    "            , qualified_counts as\n",
    "            (\n",
    "              select\n",
    "                date_trunc('hour', createddate_) as q_time\n",
    "                , count(*) as q\n",
    "              from\n",
    "                qualified\n",
    "              group by\n",
    "                q_time\n",
    "            )\n",
    "            , bankverified as\n",
    "            (\n",
    "              select\n",
    "                c_ash.application\n",
    "                , min(c_ash.createddate at time zone 'America/Chicago') as createddate_\n",
    "              from\n",
    "                cloudlending.application_status_history as c_ash\n",
    "                inner join\n",
    "                  time_limit_gmt\n",
    "                  on TRUE\n",
    "                inner join\n",
    "                  (\n",
    "                    select\n",
    "                      application\n",
    "                    from\n",
    "                      (\n",
    "                        select\n",
    "                          *\n",
    "                          , row_number() over (partition by application order by createddate desc) as rn\n",
    "                        from\n",
    "                          cloudlending.microbilt_information\n",
    "                        inner join\n",
    "                          time_limit_gmt\n",
    "                          on TRUE\n",
    "                      where\n",
    "                        createddate < createddate_\n",
    "                      ) as c_mb\n",
    "                    where\n",
    "                      rn = 1\n",
    "                      and c_mb.ibv_source = '\", ibv.provider, \"'\n",
    "                  ) as c_mb\n",
    "                  on c_ash.application = c_mb.application\n",
    "              where\n",
    "                createddate < createddate_\n",
    "                and c_ash.new_value = 'BANK VERIFICATION COMPLETED'\n",
    "                or\n",
    "                  (\n",
    "                    c_ash.old_value = 'BANK VERIFICATION COMPLETED' \n",
    "                    and c_ash.new_value = 'NEW - SCORECARD GENERATED'\n",
    "                  )\n",
    "              group by\n",
    "                c_ash.application\n",
    "            )\n",
    "            , bankverified_counts as\n",
    "            (\n",
    "              select\n",
    "                date_trunc('hour', createddate_) as bv_time\n",
    "                , count(*) as bv\n",
    "              from\n",
    "                bankverified\n",
    "              group by\n",
    "                bv_time\n",
    "            )\n",
    "            select\n",
    "              q_time::date as dayofyear\n",
    "              , extract(dow from q_time) as dayofweek\n",
    "              , extract(hour from q_time) as hourofday\n",
    "              , coalesce(q,0) as q\n",
    "              , coalesce(bv,0) as bv\n",
    "              , 'training' as grouping\n",
    "            from\n",
    "              qualified_counts as qc\n",
    "              left join\n",
    "                bankverified_counts as bvc\n",
    "                on qc.q_time = bvc.bv_time\n",
    "            where\n",
    "              q_time::date >= '\", date.string, \"'::date - '3 months'::interval\n",
    "              and q_time::date < '\", date.string, \"'::date\n",
    "\n",
    "            union all\n",
    "\n",
    "\n",
    "            select\n",
    "              q_time::date as dayofyear\n",
    "              , extract(dow from q_time) as dayofweek\n",
    "              , extract(hour from q_time) as hourofday\n",
    "              , coalesce(q,0) as q\n",
    "              , coalesce(bv,0) as bv\n",
    "              , 'test' as grouping\n",
    "            from\n",
    "              qualified_counts as qc\n",
    "              left join\n",
    "                bankverified_counts as bvc\n",
    "                on qc.q_time = bvc.bv_time\n",
    "            where\n",
    "              q_time::date = '\", date.string, \"'::date\n",
    "\n",
    "            order by\n",
    "              dayofyear asc\n",
    "              , hourofday asc\n",
    "            \",\n",
    "            sep = ''\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    return(df)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# ## 2018-12-18 -- Wells Fargo DL Outage\n",
    "# # df = read.csv('C:/Users/jchang/Desktop/Projects/Funnel Anomaly Detection/data/dataexportdlreformat_2019-1-27_0049.csv')\n",
    "\n",
    "# ## 2018-12-20 -- Everything OK\n",
    "# # df = read.csv('C:/Users/jchang/Desktop/Projects/Funnel Anomaly Detection/data/dataexportdlreformat_2019-1-26_0857.csv')\n",
    "\n",
    "# ## 2010-01-01 -- Complete DL Outage\n",
    "# # df = read.csv('C:/Users/jchang/Desktop/Projects/Funnel Anomaly Detection/data/dataexportdlreformat_2019-1-26_0900.csv')\n",
    "# # df = read.csv('C:/Users/jchang/Desktop/Projects/Funnel Anomaly Detection/data/dataexportdlreformat_2019-1-27_0007.csv')\n",
    "\n",
    "# ## 2019-01-04 -- Chase DL Errors\n",
    "# # df = read.csv('C:/Users/jchang/Desktop/Projects/Funnel Anomaly Detection/data/dataexportdlreformat_2019-1-26_1130.csv')\n",
    "# # df = read.csv('C:/Users/jchang/Desktop/Projects/Funnel Anomaly Detection/data/dataexportdlreformat_2019-1-27_0016.csv')\n",
    "\n",
    "\n",
    "# ## 2019-01-05 -- Chase DL Errors\n",
    "# # df = read.csv('C:/Users/jchang/Desktop/Projects/Funnel Anomaly Detection/data/dataexportdlreformat_2019-1-26_1527.csv')\n",
    "# # df = read.csv('C:/Users/jchang/Desktop/Projects/Funnel Anomaly Detection/data/dataexportdlreformat_2019-1-27_0011.csv')\n",
    "\n",
    "# ## 2019-01-06 -- Chase DL Errors\n",
    "# # df = read.csv('C:/Users/jchang/Desktop/Projects/Funnel Anomaly Detection/data/dataexportdlreformat_2019-1-26_1125.csv')\n",
    "# # df = read.csv('C:/Users/jchang/Desktop/Projects/Funnel Anomaly Detection/data/dataexportdlreformat_2019-1-27_0012.csv')\n",
    "\n",
    "# ## 2019-01-07 -- Chase DL Errors\n",
    "# # df = read.csv('C:/Users/jchang/Desktop/Projects/Funnel Anomaly Detection/data/dataexportdlreformat_2019-1-26_1103.csv')\n",
    "# # df = read.csv('C:/Users/jchang/Desktop/Projects/Funnel Anomaly Detection/data/dataexportdlreformat_2019-1-27_0014.csv')\n",
    "\n",
    "\n",
    "# ## 2019-01-08 -- Chase DL Errors\n",
    "# # df = read.csv('C:/Users/jchang/Desktop/Projects/Funnel Anomaly Detection/data/dataexportdlreformat_2019-1-26_1117.csv')\n",
    "# # df = read.csv('C:/Users/jchang/Desktop/Projects/Funnel Anomaly Detection/data/dataexportdlreformat_2019-1-27_0017.csv')\n",
    "\n",
    "\n",
    "# ## 2019-01-13 -- NFCU DL Errors\n",
    "# # df = read.csv('C:/Users/jchang/Desktop/Projects/Funnel Anomaly Detection/data/dataexportdlreformat_2019-1-27_0047.csv')\n",
    "\n",
    "\n",
    "# ## 2019-01-17 -- Everything OK\n",
    "# # df = read.csv('C:/Users/jchang/Desktop/Projects/Funnel Anomaly Detection/data/dataexportdlreformat_2019-1-26_1449.csv')\n",
    "\n",
    "# ## 2019-01-18 -- Everything OK\n",
    "# # df = read.csv('C:/Users/jchang/Desktop/Projects/Funnel Anomaly Detection/data/dataexportdlreformat_2019-1-26_1445.csv')\n",
    "\n",
    "# ## 2019-01-19 -- Everything OK\n",
    "# # df = read.csv('C:/Users/jchang/Desktop/Projects/Funnel Anomaly Detection/data/dataexportdlreformat_2019-1-26_1443.csv')\n",
    "\n",
    "# ## 2019-01-20 -- Everything OK\n",
    "# # df = read.csv('C:/Users/jchang/Desktop/Projects/Funnel Anomaly Detection/data/dataexportdlreformat_2019-1-26_1438.csv')\n",
    "\n",
    "# ## 2019-01-21 -- Everything OK\n",
    "# # df = read.csv('C:/Users/jchang/Desktop/Projects/Funnel Anomaly Detection/data/dataexportdlreformat_2019-1-26_1433.csv')\n",
    "\n",
    "# ## 2019-01-22 -- Everything OK\n",
    "# # df = read.csv('C:/Users/jchang/Desktop/Projects/Funnel Anomaly Detection/data/dataexportdlreformat_2019-1-26_1429.csv')\n",
    "\n",
    "# # 2019-01-23 -- Everything OK\n",
    "# # df = read.csv('C:/Users/jchang/Desktop/Projects/Funnel Anomaly Detection/data/dataexportdlreformat_2019-1-26_1428.csv')\n",
    "# df = read.csv('C:/Users/jchang/Desktop/Projects/Funnel Anomaly Detection/data/dataexportdlreformat_2019-1-27_0044.csv')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# df$dayofyear = as.Date(df$dayofyear)\n",
    "# df$grouping = as.character(df$grouping)\n",
    "\n",
    "# head(df)\n",
    "# str(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clean the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Information on the \"Session_Date\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Input data frame.\n",
    "## Output list of DOY, DOW, HOURS.\n",
    "\n",
    "getSessionInfo = function(data) {\n",
    "    \n",
    "    df = data\n",
    "    \n",
    "    current.date = tail(\n",
    "        df$dayofyear,\n",
    "        1\n",
    "    )\n",
    "\n",
    "    current.day = tail(\n",
    "        df$dayofweek,\n",
    "        1\n",
    "    )\n",
    "\n",
    "    current.hours = seq(\n",
    "        from = 0,\n",
    "        to = 23,\n",
    "        by = 1\n",
    "    )\n",
    "\n",
    "    session.info = list(\n",
    "        date = current.date,\n",
    "        dow = current.day,\n",
    "        hours = current.hours\n",
    "    )\n",
    "    \n",
    "    return(session.info)\n",
    "    \n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Aggregated Data for the \"Session_Date\" (Test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Input data frame.\n",
    "## Output aggregated (cumulative) data for TEST DATA.\n",
    "\n",
    "getSessionData = function(data, cumulative) {\n",
    "    \n",
    "    ## Create local variable on the parameter:DATA \n",
    "    df = data\n",
    "    df.session = df[which(df$grouping == 'test'), ]\n",
    "\n",
    "    \n",
    "    if (cumulative == TRUE) {\n",
    "        \n",
    "        session.point.estimates = \n",
    "            df.session %>%\n",
    "            group_by(\n",
    "                dayofyear\n",
    "            ) %>%\n",
    "    #         mutate(\n",
    "            transmute(\n",
    "                hourofday = hourofday,\n",
    "                q.session = cumsum(q),\n",
    "                bv.session = cumsum(bv),\n",
    "                ratio.session = ifelse(\n",
    "                    cumsum(q) > 0,\n",
    "                    cumsum(bv)/cumsum(q),\n",
    "                    0\n",
    "                )\n",
    "            )\n",
    "    } else if (cumulative == FALSE) {\n",
    "        \n",
    "        session.point.estimates = \n",
    "        df.session %>%\n",
    "        group_by(\n",
    "            dayofyear,\n",
    "            hourofday\n",
    "        ) %>%\n",
    "        summarize(\n",
    "            q.session = sum(q),\n",
    "            bv.session = sum(bv),\n",
    "            ratio.session = ifelse(\n",
    "                sum(q) > 0,\n",
    "                sum(bv)/sum(q),\n",
    "                0\n",
    "            )\n",
    "        )\n",
    "    }\n",
    "    \n",
    "    return(session.point.estimates)\n",
    "}\n",
    "\n",
    "# str(getSessionDataCumulative())\n",
    "# getSessionDataCumulative(data = df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Aggregated Data for History (Training)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Make Cumulative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Input data frame.\n",
    "## Output mutated (cumulative) data for TRAINING DATA.\n",
    "## Reliant on getSessionInfo().\n",
    "\n",
    "makeCumulative = function(data) {\n",
    "    \n",
    "    df = data\n",
    "    \n",
    "    df.hist = df[which(df$grouping == 'training'), ]\n",
    "    \n",
    "    df.hist.mutate = \n",
    "        df.hist[which(df.hist$dayofweek == getSessionInfo(data = df)$dow), ] %>%\n",
    "        group_by(\n",
    "            dayofyear\n",
    "        ) %>%\n",
    "        transmute(\n",
    "            dayofweek = dayofweek,\n",
    "            hourofday = hourofday,\n",
    "            q = cumsum(q),\n",
    "            bv = cumsum(bv)\n",
    "        )\n",
    "    \n",
    "    return(df.hist.mutate)\n",
    "    \n",
    "}\n",
    "\n",
    "# makeCumulative(data = df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Aggregate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Input data frame.\n",
    "## Output aggregated (cumulative) data for TRAINNG DATA.\n",
    "## Reliant on makeCumulative().\n",
    "\n",
    "getHistoricalData = function(data, cumulative) {\n",
    "    \n",
    "    ## Create local variable on the parameter:DATA     \n",
    "    df = data\n",
    "    \n",
    "    \n",
    "    if (cumulative == TRUE) {\n",
    "    \n",
    "        df.hist.mutate = makeCumulative(data = df)\n",
    "\n",
    "        historical.point.estimates = \n",
    "            df.hist.mutate %>%\n",
    "            group_by(\n",
    "                hourofday\n",
    "            ) %>%\n",
    "            summarize(\n",
    "                q.hist = sum(q),\n",
    "                bv.hist = sum(bv),\n",
    "                r.hist = sum(bv)/sum(q),\n",
    "                n.hist = sum(q),\n",
    "                sd.q.hist = sd(q),\n",
    "                sd.bv.hist = sd(bv),\n",
    "                mu.q.hist = mean(q),\n",
    "                corr.hist = cor(q, bv),\n",
    "                se.hist = sqrt(\n",
    "                    (\n",
    "                        (\n",
    "                            sd(bv)^2 + \n",
    "                            sd(q)^2 * (sum(bv)/sum(q))^2 -\n",
    "                            2 * sum(bv)/sum(q) * cor(q, bv) * sd(bv) * sd(q)\n",
    "                        ) /\n",
    "                        (\n",
    "                            sum(q) *\n",
    "                            mean(q)^2\n",
    "                        )\n",
    "                    )\n",
    "                )\n",
    "            )\n",
    "    } else if (cumulative == FALSE) {\n",
    "\n",
    "        df.hist = df[which(df$grouping == 'training'), ]\n",
    "\n",
    "        historical.point.estimates = \n",
    "            df.hist[which(df.hist$dayofweek == getSessionInfo(data = df)$dow), ] %>%\n",
    "            group_by(\n",
    "                hourofday\n",
    "            ) %>%\n",
    "            summarize(\n",
    "                q.hist = sum(q),\n",
    "                bv.hist = sum(bv),\n",
    "                r.hist = sum(bv)/sum(q),\n",
    "                n.hist = sum(q),\n",
    "                sd.q.hist = sd(q),\n",
    "                sd.bv.hist = sd(bv),\n",
    "                mu.q.hist = mean(q),\n",
    "                corr.hist = cor(q, bv),\n",
    "                se.hist = sqrt(\n",
    "                    (\n",
    "                        (\n",
    "                            sd(bv)^2 + \n",
    "                            sd(q)^2 * (sum(bv)/sum(q))^2 -\n",
    "                            2 * sum(bv)/sum(q) * cor(q, bv) * sd(bv) * sd(q)\n",
    "                        ) /\n",
    "                        (\n",
    "                            sum(q) *\n",
    "                            mean(q)^2\n",
    "                        )\n",
    "                    )\n",
    "                )\n",
    "            )\n",
    "    }\n",
    "\n",
    "    return(historical.point.estimates)\n",
    "}\n",
    "\n",
    "\n",
    "# str(getHistoricalDataCumulative(data = df))\n",
    "# rbind(head(getHistoricalData(),3), tail(getHistoricalData(),3))\n",
    "# getHistoricalDataCumulative(data = df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run the bootstrap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Construct Data Frame to store final information for ggplot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Input data frame.\n",
    "## Output Initialized CI data frame.\n",
    "## Reliant on getSessionDataCumulative() and getHistoricalDataCumulative().\n",
    "\n",
    "getInitialCI = function(data, cumulative) {\n",
    "    \n",
    "    df = data\n",
    "    \n",
    "    df.session = getSessionData(data = df, cumulative = cumulative)\n",
    "    df.historical = getHistoricalData(data = df, cumulative = cumulative)\n",
    "\n",
    "    ci.information = data.frame(\n",
    "\n",
    "        date.session = rep(max(df.session$dayofyear),24),\n",
    "        hour.session = df.historical$hourofday,\n",
    "        r.session = c(df.session$ratio.session, rep(0, 24 - length(df.session$ratio.session))),\n",
    "        r.hist = df.historical$r.hist,\n",
    "        se.hist = df.historical$se.hist,\n",
    "        z.lower = rep(0,24),\n",
    "        z.upper = rep(0,24),\n",
    "        ci.lower = rep(0,24),\n",
    "        ci.upper = rep(0,24)\n",
    "    )\n",
    "    \n",
    "    return(ci.information)\n",
    "}\n",
    "\n",
    "# getInitialCI(data = df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Execute the Bootstrap."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Input data frame and Bootstrap parameters.\n",
    "## Output Finalized CI data frame.\n",
    "## Reliant on getInitialCI() and makeCumulative().\n",
    "\n",
    "getBootstrapInterval = function(data, cumulative, B, alpha) {\n",
    "    \n",
    "    df = data\n",
    "    \n",
    "#     ## Set Start Time.\n",
    "#     start_time = Sys.time()\n",
    "    \n",
    "    ## Initialize final output data frame.\n",
    "    ci.information = getInitialCI(data = df, cumulative = cumulative)\n",
    "    \n",
    "    \n",
    "    \n",
    "    ## Initialize data for bootstrap.\n",
    "    if (cumulative == TRUE) {\n",
    "        \n",
    "        df.training.dow = makeCumulative(data = df)\n",
    "    } else if (cumulative == FALSE) {\n",
    "        \n",
    "        df.training = df[which(df$grouping == 'training'), ]\n",
    "        df.training.dow = df.training[which(df.training$dayofweek == getSessionInfo(data = df)$dow), ]\n",
    "    }\n",
    "    \n",
    "        \n",
    "    \n",
    "    ## Initialize container for bootstrap.\n",
    "    bootstrap_z = vector(length = B)\n",
    "    \n",
    "    \n",
    "    for (i in 1:24) {\n",
    "\n",
    "        for (j in 1:B) {\n",
    "\n",
    "\n",
    "\n",
    "            ## Subset the data to the current HOD (i).\n",
    "            df.training.dow.hour = df.training.dow[which(df.training.dow$hourofday == i - 1), ]\n",
    "\n",
    "            index = seq(\n",
    "                1, \n",
    "                nrow(df.training.dow.hour)\n",
    "            )\n",
    "            resample_index = sample(\n",
    "                x = index, \n",
    "                size = length(index), \n",
    "                replace = TRUE\n",
    "            )\n",
    "            df.training.dow.hour.resample = df.training.dow.hour[resample_index, ]\n",
    "\n",
    "\n",
    "            ## Calculate statistics from the resample.\n",
    "            r = sum(df.training.dow.hour.resample$bv)/sum(df.training.dow.hour.resample$q)\n",
    "            n = sum(df.training.dow.hour.resample$q)\n",
    "            sx = sd(df.training.dow.hour.resample$q)\n",
    "            sy = sd(df.training.dow.hour.resample$bv)\n",
    "            mx = mean(df.training.dow.hour.resample$q)\n",
    "            corr = cor(df.training.dow.hour.resample$bv, df.training.dow.hour.resample$q)\n",
    "\n",
    "            se = sqrt(\n",
    "                (r^2*sx^2 + sy^2 - 2*r*corr*sx*sy)/\n",
    "                (n*mx^2)\n",
    "            )\n",
    "\n",
    "\n",
    "            ## Calculate statistics from the training data.\n",
    "            mu = ci.information$r.hist[i]\n",
    "\n",
    "\n",
    "            ## Calculate the bootstrap Z\n",
    "            bootstrap_z[j] = (r - mu)/se\n",
    "        }\n",
    "\n",
    "\n",
    "        ## For each hour, take Percentiles of the Bootstrap Z vector to caluclate the confidence interval for that hour.\n",
    "        bootstrap_z = sort(bootstrap_z)\n",
    "\n",
    "        ci.information$z.lower[i] = bootstrap_z[alpha/2*B]\n",
    "        ci.information$z.upper[i] = bootstrap_z[(1-alpha/2)*B]\n",
    "\n",
    "        ci.information$ci.lower = ci.information$r.hist - ci.information$z.upper * ci.information$se.hist\n",
    "        ci.information$ci.upper = ci.information$r.hist - ci.information$z.lower * ci.information$se.hist\n",
    "    }\n",
    "    \n",
    "    return(ci.information)\n",
    "    \n",
    "#     elapsed_time = Sys.time() - start_time\n",
    "    \n",
    "#     return(\n",
    "#         list(\n",
    "#             ci.information = ci.information, \n",
    "#             elapsed_time = elapsed_time\n",
    "#         )\n",
    "#     )\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Smooth the CI.Lower Line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "smoothLowerBound = function(ci.information, iterations) {\n",
    "    \n",
    "    ci.information.smooth = ci.information\n",
    "    \n",
    "    for (i in 1:iterations) {\n",
    "    \n",
    "        for (j in (0+1):(23-1)) {\n",
    "\n",
    "            ci.information.smooth$ci.lower[which(ci.information.smooth$hour.session == j)] = mean(\n",
    "                c(ci.information.smooth$ci.lower[which(ci.information.smooth$hour.session == j)-1], ci.information.smooth$ci.lower[which(ci.information.smooth$hour.session == j)+1])\n",
    "            )\n",
    "        }        \n",
    "    }\n",
    "    \n",
    "    return(ci.information.smooth)\n",
    "\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create the ggplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "getBootstrapPlot = function(df.bi, ibv.provider) {\n",
    "    \n",
    "    df = df.bi\n",
    "    \n",
    "    ggplot(\n",
    "        data = df,\n",
    "        mapping = aes(\n",
    "            x = hour.session, \n",
    "            y = r.session\n",
    "        )        \n",
    "    ) +\n",
    "    geom_col(\n",
    "        fill = 'sky blue'\n",
    "    ) + \n",
    "    labs(\n",
    "        x =\"Time of Day\", \n",
    "        y = \"Success/Q\", \n",
    "        title = paste(\n",
    "            ibv.provider, df$date.session,\n",
    "            \"\\n\", \"Bars: Non-Cohorted DL Success/Q (Cumulative)\", \n",
    "            \"\\n\", \"Line: 3% lower bound confidence interval of bars\"\n",
    "        )\n",
    "    ) + \n",
    "    geom_line(\n",
    "        aes(\n",
    "            x = hour.session, \n",
    "            y = ci.lower\n",
    "        ), \n",
    "        color = 'red', \n",
    "        size=1\n",
    "    ) + \n",
    "    # geom_line(\n",
    "    #     aes(\n",
    "    #         x = hour.session, \n",
    "    #         y = ci.upper\n",
    "    #     ), \n",
    "    #     color = 'blue', \n",
    "    #     size=1\n",
    "    # ) + \n",
    "    scale_y_continuous(\n",
    "        labels = scales::percent \n",
    "    ) + \n",
    "    scale_x_continuous(\n",
    "        breaks = seq(0,23,4)\n",
    "    )    \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1]]\n",
      "<PostgreSQLConnection>\n",
      "\n"
     ]
    },
    {
     "data": {},
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Time difference of 1.442057 mins"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "start = Sys.time()\n",
    "\n",
    "getBootstrapPlot(\n",
    "    df.bi = smoothLowerBound(\n",
    "        ci.information = getBootstrapInterval(\n",
    "            data = getDf(\n",
    "                date.string = Sys.Date(), \n",
    "                ibv.provider = 'DecisionLogic'\n",
    "            ),\n",
    "            cumulative = TRUE,\n",
    "            B = 5000,\n",
    "            alpha = 0.03\n",
    "        ),\n",
    "        iterations = 2\n",
    "    ),\n",
    "    ibv.provider = 'DecisionLogic'\n",
    ")\n",
    "\n",
    "end = Sys.time()\n",
    "end - start\n",
    "\n",
    "start = Sys.time()\n",
    "\n",
    "getBootstrapPlot(\n",
    "    df.bi = smoothLowerBound(\n",
    "        ci.information = getBootstrapInterval(\n",
    "            data = getDf(\n",
    "                date.string = Sys.Date(), \n",
    "                ibv.provider = 'MicroBilt'\n",
    "            ),\n",
    "            cumulative = TRUE,\n",
    "            B = 5000,\n",
    "            alpha = 0.03\n",
    "        ),\n",
    "        iterations = 2\n",
    "    ),\n",
    "    ibv.provider = 'MicroBilt'\n",
    ")\n",
    "\n",
    "end = Sys.time()\n",
    "end - start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.bi = getBootstrapInterval(\n",
    "    data = df,\n",
    "    B = 5000,\n",
    "    alpha = 0.03\n",
    ")$ci.information\n",
    "\n",
    "ggplot() +\n",
    "geom_line(\n",
    "    data = df.bi,\n",
    "    mapping = aes(\n",
    "        x = hour.session,\n",
    "        y = r.session\n",
    "    ),\n",
    "    color = 'skyblue',\n",
    "    size = 3\n",
    ") +\n",
    "geom_line(\n",
    "    data = df.bi,\n",
    "    mapping = aes(\n",
    "        x = hour.session,\n",
    "        y = ci.lower\n",
    "    ),\n",
    "    color = 'red',\n",
    "    size = 1,\n",
    "    linetype = 6\n",
    ") +\n",
    "geom_point(\n",
    "    data = makeCumulative(data = df),\n",
    "    mapping = aes(\n",
    "        x = hourofday,\n",
    "        y = dl.hist.cum/q.hist.cum\n",
    "    ),\n",
    "    color = 'black',\n",
    "    size = 0.8,\n",
    "    alpha = 0.1\n",
    ") +\n",
    "# geom_ribbon(\n",
    "#     data = makeCumulative(data = df) %>%\n",
    "#         group_by(\n",
    "#             hourofday\n",
    "#         ) %>%\n",
    "#         summarize(\n",
    "#             ribbon.lower = min(dl.hist.cum/q.hist.cum),\n",
    "#             ribbon.upper = max(dl.hist.cum/q.hist.cum)\n",
    "#         ), \n",
    "#     mapping = aes(\n",
    "#         x = hourofday,\n",
    "#         ymin = ribbon.lower,\n",
    "#         ymax = ribbon.upper\n",
    "#     ),\n",
    "#     color = 'black',\n",
    "#     size = 0.8,\n",
    "#     alpha = 0.1\n",
    "# ) +\n",
    "scale_y_continuous(\n",
    "    labels = scales::percent,\n",
    "#     limits = c(\n",
    "#         min(dl.hist.cum/q.hist.cum),\n",
    "#         max(dl.hist.cum/q.hist.cum),\n",
    "#     ) \n",
    "    limits = c(0.20,0.70)\n",
    ") \n",
    "\n",
    "\n",
    "# + \n",
    "# labs(\n",
    "#     x =\"Time of Day\", \n",
    "#     y = \"Success/Q\", \n",
    "#     title = paste(\n",
    "#         \"DL \", getSessionInfo(data = df)$date,\n",
    "#         \"\\n\", \"Bars: Non-Cohorted DL Success/Q (Cumulative)\", \n",
    "#         \"\\n\", \"Line: 5% lower bound confidence interval of bars\")\n",
    "# ) + \n",
    "# geom_line(\n",
    "#     aes(\n",
    "#         x = hour.session, \n",
    "#         y = ci.lower\n",
    "#     ), \n",
    "#     color = 'red', \n",
    "#     size=1\n",
    "# ) + \n",
    "# # geom_line(\n",
    "# #     aes(\n",
    "# #         x = hour.session, \n",
    "# #         y = ci.upper\n",
    "# #     ), \n",
    "# #     color = 'blue', \n",
    "# #     size=1\n",
    "# # ) + \n",
    "# scale_x_continuous(\n",
    "#     breaks = seq(0,23,4)\n",
    "# )# + geom_line(aes(x=hourofday, y=upper), color='springgreen4', size=1) + scale_y_continuous(labels=scales::percent) + scale_x_continuous(breaks=seq(0,23,4))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
